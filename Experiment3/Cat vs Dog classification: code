# Import necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader, random_split
import matplotlib.pyplot as plt
import os
import PIL
import pandas as pd
from sklearn.model_selection import ParameterGrid

# Device configuration
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define data preprocessing
transform = transforms.Compose([
    transforms.Resize((128, 128)),  
    transforms.ToTensor(),
    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  
])

# Correct dataset path
dataset_path = "/content/dataset/PetImages"

# Remove corrupt files
def clean_dataset(root_path):
    for category in ["Cat", "Dog"]:
        folder_path = os.path.join(root_path, category)
        for file in os.listdir(folder_path):
            file_path = os.path.join(folder_path, file)
            try:
                img = PIL.Image.open(file_path)
                img.verify()
            except (PIL.UnidentifiedImageError, IOError, OSError):
                print(f"Removing corrupt file: {file_path}")
                os.remove(file_path)

clean_dataset(dataset_path)

# Load dataset
full_dataset = datasets.ImageFolder(dataset_path, transform=transform)

# Split into train & validation (80% train, 20% validation)
train_size = int(0.8 * len(full_dataset))
val_size = len(full_dataset) - train_size
train_data, val_data = random_split(full_dataset, [train_size, val_size])

# Create DataLoader
train_loader = DataLoader(train_data, batch_size=32, shuffle=True)
val_loader = DataLoader(val_data, batch_size=32, shuffle=False)

print("Classes:", full_dataset.classes)
print("Total Images:", len(full_dataset))
print("Train Images:", len(train_data))
print("Validation Images:", len(val_data))

# Define CNN Model
class CNN(nn.Module):
    def __init__(self, activation):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.fc1 = nn.Linear(64 * 32 * 32, 128)
        self.fc2 = nn.Linear(128, 1)  
        self.activation = activation()
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)

    def forward(self, x):
        x = self.pool(self.activation(self.conv1(x)))
        x = self.pool(self.activation(self.conv2(x)))
        x = x.view(x.size(0), -1)
        x = self.activation(self.fc1(x))
        x = self.fc2(x)
        return torch.sigmoid(x)

# Initialize weights
def initialize_weights(model, init_type):
    for layer in model.modules():
        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):
            if init_type == "xavier":
                nn.init.xavier_uniform_(layer.weight)
            elif init_type == "kaiming":
                nn.init.kaiming_uniform_(layer.weight, nonlinearity="relu")
            elif init_type == "random":
                nn.init.uniform_(layer.weight, a=-0.1, b=0.1)

# Train and evaluate CNN
def train_and_evaluate_cnn(params):
    activation = params["activation"]
    weight_init = params["weight_init"]
    optimizer_type = params["optimizer"]

    model = CNN(activation).to(device)
    initialize_weights(model, weight_init)

    criterion = nn.BCELoss()
        
    
    if optimizer_type == "sgd":
        optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
    elif optimizer_type == "adam":
        optimizer = optim.Adam(model.parameters(), lr=0.001)
    elif optimizer_type == "rmsprop":
        optimizer = optim.RMSprop(model.parameters(), lr=0.001)


    train_losses, val_losses, train_acc, val_acc = [], [], [], []

    for epoch in range(5):
        model.train()
        correct, train_loss = 0, 0
        for images, labels in train_loader:
            images, labels = images.to(device), labels.float().unsqueeze(1).to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
            preds = (outputs > 0.5).float()
            correct += (preds == labels).sum().item()

        accuracy = correct / len(train_data)
        train_losses.append(train_loss / len(train_loader))
        train_acc.append(accuracy)

        # Validation
        model.eval()
        correct, val_loss = 0, 0
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.float().unsqueeze(1).to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)
                val_loss += loss.item()
                preds = (outputs > 0.5).float()
                correct += (preds == labels).sum().item()

        val_losses.append(val_loss / len(val_loader))
        val_acc.append(correct / len(val_data))

        print(f"[CNN] Epoch {epoch+1}: Train Loss={train_losses[-1]:.4f}, Val Loss={val_losses[-1]:.4f}, Train Acc={train_acc[-1]:.4f}, Val Acc={val_acc[-1]:.4f}")

    # Plot results
    plt.figure(figsize=(10, 4))
    plt.subplot(1, 2, 1)
    plt.plot(range(1, 6), train_losses, label="Train Loss")
    plt.plot(range(1, 6), val_losses, label="Validation Loss")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(range(1, 6), train_acc, label="Train Accuracy")
    plt.plot(range(1, 6), val_acc, label="Validation Accuracy")
    plt.xlabel("Epochs")
    plt.ylabel("Accuracy")
    plt.legend()
    plt.show()

    return val_losses[-1], val_acc[-1], model

# Run hyperparameter tuning
param_grid = {
    "activation": [nn.ReLU, nn.Tanh, nn.LeakyReLU],
    "weight_init": ["xavier", "kaiming", "random"],
    "optimizer": ["sgd", "adam", "rmsprop"]
}

results = []
for params in ParameterGrid(param_grid):
    print(f"Testing CNN combination: {params}")
    val_loss, val_acc, model = train_and_evaluate_cnn(params)
    results.append([params["activation"].__name__, params["weight_init"], params["optimizer"], val_loss, val_acc])

df = pd.DataFrame(results, columns=["Activation", "Weight Init", "Optimizer", "Validation Loss", "Validation Accuracy"])
print(df)

# Fine-tune ResNet-18
resnet18 = models.resnet18(pretrained=True)
num_features = resnet18.fc.in_features
resnet18.fc = nn.Sequential(nn.Linear(num_features, 1), nn.Sigmoid())
resnet18 = resnet18.to(device)

optimizer = optim.Adam(resnet18.parameters(), lr=0.001)
train_and_evaluate_cnn({"activation": nn.ReLU, "weight_init": "xavier", "optimizer": "adam"})

torch.save(resnet18.state_dict(), "resnet18_model.pth")

# Compare CNN vs. ResNet-18
print(df.sort_values(by="Validation Accuracy", ascending=False).head(1))
