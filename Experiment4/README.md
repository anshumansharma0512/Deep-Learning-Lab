#ðŸ“œ Text Generation Using RNNs and LSTMs  

##ðŸ”¹ **Overview**  
This project explores **text generation using Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks**.  
We compare two encoding techniques:  
1. **One-Hot Encoding**  
2. **Trainable Word Embeddings**  

The goal is to **train an RNN and LSTM model** on a dataset of 100 poems and compare their performance.  

---

## **ðŸ“‚ Dataset**  
The dataset (`poems-100.csv`) contains **100 poems**. The text is preprocessed and tokenized before training.  

---

## **ðŸ“Œ Implementation Details**  

### **ðŸ”¹ Model Variants**  
We implement **four models**:  
âœ… **RNN with One-Hot Encoding**  
âœ… **LSTM with One-Hot Encoding**  
âœ… **RNN with Trainable Embeddings**  
âœ… **LSTM with Trainable Embeddings**

Model	Training   Time (s)	  Average Loss
RNN One-Hot	     ~50s	        ~2.3
LSTM One-Hot	   ~70s	        ~1.8
RNN Embeddings	 ~90s	        ~1.0
LSTM Embeddings	 ~115s	      ~0.5

ðŸ”¹ ***Advantages & Disadvantages of Each Approach***
ðŸ“Œ**One-Hot Encoding**
âœ… Advantages:
->Simple and interpretable
->Works well with small datasets

âŒ Disadvantages:
->Very memory-intensive (high-dimensional vectors)
->Fails to capture word relationships

ðŸ“Œ **Trainable Word Embeddings**
âœ… Advantages:
->Efficient (low-dimensional representation)
->Captures semantic meaning (e.g., "king" & "queen" are related)
->Better generalization on unseen data

âŒ Disadvantages:
->Requires more training data
->Initial embeddings are random, so performance improves over time

***ðŸŽ¯ Conclusion***
->LSTMs with embeddings perform best for text generation.
->One-Hot Encoding is inefficient and struggles with large vocabularies.
->Embeddings improve contextual understanding and generalization.
->LSTM should be preferred over RNN for sequential text tasks.
