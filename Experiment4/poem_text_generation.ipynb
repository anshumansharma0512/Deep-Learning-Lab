{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPecyCHgZJyNIwiETvKP0xz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshumansharma0512/Deep-Learning-Lab/blob/main/poem_text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrHVxShWFeDE",
        "outputId": "c4fbf553-3e93-4b40-cdb3-b2978031467b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Vocabulary Size: 5405\n"
          ]
        }
      ],
      "source": [
        "# Import Necessary Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "import nltk\n",
        "import time\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Ensure required NLTK resources are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Download the punkt_tab resource\n",
        "\n",
        "# Load Dataset (Ensure Correct Column Name)\n",
        "df = pd.read_csv(\"poems-100.csv\")\n",
        "\n",
        "# Verify column name (must be 'text')\n",
        "if 'text' not in df.columns:\n",
        "    raise ValueError(\"CSV file must have a 'text' column.\")\n",
        "\n",
        "# Preprocess Text Data\n",
        "text = \" \".join(df['text'].astype(str).tolist())  # Join all poems\n",
        "words = word_tokenize(text.lower())  # Tokenize & lowercase\n",
        "\n",
        "# Create Vocabulary\n",
        "word_counts = Counter(words)\n",
        "vocab = sorted(word_counts.keys())  # Sorted unique words\n",
        "word2idx = {word: i for i, word in enumerate(vocab)}\n",
        "idx2word = {i: word for word, i in word2idx.items()}\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "print(f\" Vocabulary Size: {vocab_size}\")\n",
        "\n",
        "# Define Sequence Length\n",
        "sequence_length = 10  # Increased for better context\n",
        "\n",
        "# One-Hot Encoding Function\n",
        "def one_hot_encode(word, vocab_size, word2idx):\n",
        "    vector = torch.zeros(vocab_size)\n",
        "    if word in word2idx:\n",
        "        vector[word2idx[word]] = 1\n",
        "    return vector\n",
        "\n",
        "# Prepare Data for One-Hot Encoding\n",
        "def prepare_one_hot_data(words, vocab_size, word2idx, sequence_length):\n",
        "    train_X, train_y = [], []\n",
        "    for i in range(len(words) - sequence_length):\n",
        "        context = words[i:i+sequence_length]\n",
        "        target = words[i+sequence_length]\n",
        "        train_X.append(torch.stack([one_hot_encode(w, vocab_size, word2idx) for w in context]))\n",
        "        train_y.append(word2idx[target])\n",
        "    return torch.stack(train_X), torch.tensor(train_y)\n",
        "\n",
        "train_X_one_hot, train_y_one_hot = prepare_one_hot_data(words, vocab_size, word2idx, sequence_length)\n",
        "\n",
        "# Prepare Data for Word Embeddings\n",
        "def prepare_indexed_data(words, word2idx, sequence_length):\n",
        "    train_X, train_y = [], []\n",
        "    for i in range(len(words) - sequence_length):\n",
        "        context = words[i:i+sequence_length]\n",
        "        target = words[i+sequence_length]\n",
        "        train_X.append([word2idx[w] for w in context])\n",
        "        train_y.append(word2idx[target])\n",
        "    return torch.tensor(train_X), torch.tensor(train_y)\n",
        "\n",
        "train_X_index, train_y_index = prepare_indexed_data(words, word2idx, sequence_length)\n",
        "\n",
        "# Define RNN Model\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_dim, output_size, num_layers, dropout=0.3):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.rnn = nn.RNN(input_size, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, hidden = self.rnn(x)\n",
        "        return self.fc(hidden[-1])\n",
        "\n",
        "# Define LSTM Model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_dim, output_size, num_layers, dropout=0.3):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (hidden, _) = self.lstm(x)\n",
        "        return self.fc(hidden[-1])\n",
        "\n",
        "# Define RNN Model with Embeddings\n",
        "class RNNEmbeddingModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_size, num_layers, dropout=0.3):\n",
        "        super(RNNEmbeddingModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn = nn.RNN(embed_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        _, hidden = self.rnn(x)\n",
        "        return self.fc(hidden[-1])\n",
        "\n",
        "# Define LSTM Model with Embeddings\n",
        "class LSTMEmbeddingModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_size, num_layers, dropout=0.3):\n",
        "        super(LSTMEmbeddingModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        _, (hidden, _) = self.lstm(x)\n",
        "        return self.fc(hidden[-1])\n",
        "\n",
        "# Train Function (Tracks Accuracy & Training Time)\n",
        "def train_model(model, train_X, train_y, num_epochs=20, batch_size=32, learning_rate=0.001):\n",
        "    dataset = torch.utils.data.TensorDataset(train_X, train_y)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    total_start_time = time.time()\n",
        "    all_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_start_time = time.time()\n",
        "        total_loss, correct, total = 0, 0, 0\n",
        "        for inputs, targets in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            predicted = torch.argmax(outputs, dim=1)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        accuracy = correct / total\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        all_losses.append(avg_loss)\n",
        "\n",
        "        print(f\" Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy*100:.2f}%, Time: {time.time() - epoch_start_time:.2f}s\")\n",
        "\n",
        "    total_training_time = time.time() - total_start_time\n",
        "    return sum(all_losses) / len(all_losses), total_training_time\n",
        "\n",
        "# Hyperparameters\n",
        "hidden_dim = 128\n",
        "num_layers = 2\n",
        "embed_dim = 100  # Increased embedding size for better representation\n",
        "\n",
        "# Train and Compare Models\n",
        "results = {}\n",
        "\n",
        "models = {\n",
        "    \"RNN One-Hot\": RNNModel(vocab_size, hidden_dim, vocab_size, num_layers),\n",
        "    \"LSTM One-Hot\": LSTMModel(vocab_size, hidden_dim, vocab_size, num_layers),\n",
        "    \"RNN Embeddings\": RNNEmbeddingModel(vocab_size, embed_dim, hidden_dim, vocab_size, num_layers),\n",
        "    \"LSTM Embeddings\": LSTMEmbeddingModel(vocab_size, embed_dim, hidden_dim, vocab_size, num_layers),\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n Training {name}...\")\n",
        "    if \"One-Hot\" in name:\n",
        "        avg_loss, train_time = train_model(model, train_X_one_hot, train_y_one_hot)\n",
        "    else:\n",
        "        avg_loss, train_time = train_model(model, train_X_index, train_y_index)\n",
        "\n",
        "    results[name] = {\"Average Loss\": avg_loss, \"Training Time (s)\": train_time}\n",
        "\n",
        "# Print Results\n",
        "print(\"\\n Final Results:\")\n",
        "for model, metrics in results.items():\n",
        "    print(f\"{model}: Average Loss = {metrics['Average Loss']:.4f}, Training Time = {metrics['Training Time (s)']:.2f}s\")\n"
      ]
    }
  ]
}
